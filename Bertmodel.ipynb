{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bertmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7ND2aC3MriANJFPmZsiKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunginH345/Finalproject/blob/main/Bertmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHNdr3ZXPEDT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
        "\n",
        "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
        "import random\n",
        "import os\n",
        "import io\n",
        "% matplotlib inline"
      ],
      "metadata": {
        "id": "kZL3M5DuPKjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "SEED = 19\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "cMlauMPoPKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "7wRArZuVPKn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "BgPAaff0PKqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "% cd 'gdrive/MyDrive/finalproject/archive'"
      ],
      "metadata": {
        "id": "CblP9f0TPKsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"train.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
        "df_test = pd.read_csv(\"test.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
        "df_val = pd.read_csv(\"val.txt\", delimiter=';', header=None, names=['sentence','label'])\n"
      ],
      "metadata": {
        "id": "BZD_0ydZPKuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_train,df_test,df_val])\n",
        "df['label'].unique()"
      ],
      "metadata": {
        "id": "ZVXn9XvwPKyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "df['label_enc'] = labelencoder.fit_transform(df['label'])"
      ],
      "metadata": {
        "id": "9uowzOwPPK0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'label':'label_desc'},inplace=True)\n",
        "df.rename(columns={'label_enc':'label'},inplace=True)"
      ],
      "metadata": {
        "id": "3jBvpLkKPK20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create label and sentence list\n",
        "sentences = df.sentence.values\n",
        "\n",
        "#check distribution of data based on labels\n",
        "# print(\"Distribution of data based on labels: \",df.label.value_counts())\n",
        "\n",
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 256\n",
        "\n",
        "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
        "input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n",
        "labels = df.label.values\n",
        "\n",
        "# print(\"Actual sentence before tokenization: \",sentences[2])\n",
        "# print(\"Encoded Input from dataset: \",input_ids[2])\n",
        "\n",
        "## Create attention mask\n",
        "attention_masks = []\n",
        "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
        "# print(attention_masks[2])"
      ],
      "metadata": {
        "id": "3t6liIpoPaoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
        "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)"
      ],
      "metadata": {
        "id": "Qiir4A8hParN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all our data into torch tensors, required data type for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "print(validation_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "75vEXBFNPata"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n",
        "\n",
        "# Parameters:\n",
        "lr = 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "num_warmup_steps = 0\n",
        "num_training_steps = len(train_dataloader)*epochs\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_ste"
      ],
      "metadata": {
        "id": "nVHz8p4bPav8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "learning_rate = []\n",
        "\n",
        "# Gradients gets accumulated by default\n",
        "model.zero_grad()\n",
        "\n",
        "# tnrange is a tqdm wrapper around the normal python range\n",
        "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
        "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
        "  # Calculate total loss for this epoch\n",
        "  batch_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    loss = outputs[0]\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip the norm of the gradients to 1.0\n",
        "    # Gradient clipping is not in AdamW anymore\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    \n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "    # Clear the previous accumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Update tracking variables\n",
        "    batch_loss += loss.item()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_train_loss = batch_loss / len(train_dataloader)\n",
        "\n",
        "  #store the current learning rate\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
        "    learning_rate.append(param_group['lr'])\n",
        "    \n",
        "  train_loss_set.append(avg_train_loss)\n",
        "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    print(batch)\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      \n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits[0].to('cpu').numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    labels_flat = label_ids.flatten()\n",
        "  \n",
        "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
        "    \n",
        "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
        "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
        "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')"
      ],
      "metadata": {
        "id": "dPdEEv5EPax_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## emotion labels\n",
        "label2int = {\n",
        "  \"sadness\": 4,\n",
        "  \"joy\": 2,\n",
        "  \"anger\": 0,\n",
        "  \"fear\": 1,\n",
        "  \"surprise\": 5\n",
        "}"
      ],
      "metadata": {
        "id": "xr7_6x62PtRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(predict_sentence):\n",
        "\n",
        "    inpu = [tokenizer.encode(predict_sentence, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True)]\n",
        "    labe = [3]\n",
        "\n",
        "    ## Create attention mask\n",
        "    atten_masks = []\n",
        "    ## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
        "    atten_masks = [[float(i>0) for i in seq] for seq in inpu]\n",
        "\n",
        "    # train_inputs,train_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
        "    # train_masks = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)\n",
        "\n",
        "    # convert all our data into torch tensors, required data type for our model\n",
        "    train_in = torch.tensor(inpu)\n",
        "    train_la = torch.tensor(labe)\n",
        "    train_ma = torch.tensor(atten_masks)\n",
        "\n",
        "    # # # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "    # # # # with an iterator the entire dataset does not need to be loaded into memory\n",
        "    train_da = TensorDataset(train_in,train_ma,train_la)\n",
        "    train_sam = RandomSampler(train_da)\n",
        "    train_dataloar = DataLoader(train_da,sampler=train_sam,batch_size=batch_size)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # # Evaluate data for one epoch\n",
        "    for batch in train_dataloar:\n",
        "    # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "    # # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "    # # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "    #   # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    logits = logits[0].to('cpu').numpy()\n",
        "    print(np.argmax(logits[0]))\n"
      ],
      "metadata": {
        "id": "JMQxW1C3PtTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tNpFV1tyPtVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6JAhwWfPPaz7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}